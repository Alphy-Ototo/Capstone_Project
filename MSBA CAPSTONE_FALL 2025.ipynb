{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Swire Coca-Cola Cart Abandonment Analysis**\n",
        "### MSBA Capstone  \n",
        "**Author:** Alphonsinah Ototo  \n",
        "\n",
        "---\n",
        "\n",
        "# Table of Contents  \n",
        "1. Introduction\n",
        "2. Data Sources & Loading  \n",
        "3. Exploratory Data Analysis  \n",
        "   - Customer segmentation  \n",
        "   - Ordering behavior  \n",
        "   - Google Analytics usage behavior  \n",
        "   - Operational cadence & cutoff times  \n",
        "   - Product catalog insights  \n",
        "4. Key EDA Insights  \n",
        "5. Modeling Workflow  \n",
        "   - Target definition  \n",
        "   - Leakage handling  \n",
        "   - Feature engineering  \n",
        "   - Train/test split (time-based)  \n",
        "6. Model Training & Evaluation  \n",
        "   - Logistic regression  \n",
        "   - ROC, confusion matrix, classification report  \n",
        "   - Calibration & Brier score  \n",
        "7. Profit-Based Threshold Optimization  \n",
        "8. Final Recommendations & Next Steps  \n"
      ],
      "metadata": {
        "id": "e9gG9xRThdJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "\n",
        "Swire Coca-Cola’s MyCoke360 platform enables B2B customers to browse products, place orders, and manage deliveries.  \n",
        "However, many sessions involve product interaction but do not result in a completed purchase, leading to significant cart abandonment.\n",
        "\n",
        "Cart abandonment matters because it affects:\n",
        "\n",
        "- Lost revenue  \n",
        "- Increased operational workload from manual follow-ups  \n",
        "- Inefficient fulfillment planning    \n",
        "\n",
        "This notebook contains my individual EDA and modeling work for the MSBA Capstone.  \n",
        "My goal is to:\n",
        "\n",
        "1. Understand customer behavior across several operational datasets.  \n",
        "2. Identify key predictors associated with cart abandonment.  \n",
        "3. Build a leakage-safe model to estimate purchase probability.  \n",
        "4. Use profit-based thresholding to determine which customers are worth contacting.  \n",
        "\n",
        "This notebook represents my personal contribution to the group project.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cJ8HS66dSx1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Sources & Loading\n",
        "\n",
        "The analysis uses multiple operational datasets, including:\n",
        "\n",
        "- **Customer master**\n",
        "- **Sales orders**\n",
        "- **Google Analytics event logs**\n",
        "- **Operating hours & order frequency**\n",
        "- **Cutoff times**\n",
        "- **Product catalog**\n",
        "- **Visit plan history**\n",
        "\n"
      ],
      "metadata": {
        "id": "lZNaZ9FX95Js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oeJwXZS1hiNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "data_path = \"/content/drive/MyDrive/Swire_CocaCola_Data\"\n",
        "os.listdir(data_path)"
      ],
      "metadata": {
        "id": "guim0CwviDEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "customer = pd.read_csv(data_path + \"/customer.csv\")\n",
        "sales = pd.read_csv(data_path + \"/sales.csv\")\n",
        "google_analytics = pd.read_csv(data_path + \"/google_analytics.csv\")\n",
        "operating_hours = pd.read_csv(data_path + \"/operating_hours.csv\")\n",
        "cutoff_times= pd.read_csv(data_path + \"/cutoff_times.csv\")\n",
        "orders = pd.read_csv(data_path + \"/ orders.csv\")\n",
        "materials = pd.read_csv(data_path + \"/material.csv\")\n",
        "visit_plan = pd.read_csv(data_path + \"/visit_plan.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OG2PtaoCoNTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Exploratory Data Analysis (EDA)\n",
        "This section explores customer behavior, ordering patterns, event-level engagement, and operational constraints that may contribute to cart abandonment.\n",
        "\n"
      ],
      "metadata": {
        "id": "IJRFBNOP_D6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preview\n",
        "customer.head()"
      ],
      "metadata": {
        "id": "6AOmmmomq4fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer.info()\n",
        "customer.shape"
      ],
      "metadata": {
        "id": "1ctajWisFhSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values\n",
        "customer.isnull().sum()"
      ],
      "metadata": {
        "id": "nUDf7aG8GKW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_pct = (customer['DISTRIBUTION_MODE_DESCRIPTION'].isnull().sum() / len(customer)) * 100\n",
        "print(f\"Missing values in DISTRIBUTION_MODE_DESCRIPTION: {missing_pct:.2f}%\")"
      ],
      "metadata": {
        "id": "tNqpGU5qG5Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " The dataset contains 6,334 customers with complete information across most fields.  \n",
        "Distribution mode has minimal data missing (0.06%)\n"
      ],
      "metadata": {
        "id": "A7lUp5ls_pYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping Null Values since they have a minimal effect on the dataset\n",
        "customer = customer.dropna(subset=['DISTRIBUTION_MODE_DESCRIPTION'])"
      ],
      "metadata": {
        "id": "q0FPh7Q6HiL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Duplicates\n",
        "customer.duplicated().sum()\n"
      ],
      "metadata": {
        "id": "u-3aAZveH05z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique values\n",
        "print(customer['SALES_OFFICE'].nunique())\n",
        "print(customer['SALES_OFFICE_DESCRIPTION'].nunique())\n",
        "print(customer['DISTRIBUTION_MODE_DESCRIPTION'].value_counts())\n",
        "print(customer['SHIPPING_CONDITIONS_DESCRIPTION'].value_counts())\n",
        "print(customer['COLD_DRINK_CHANNEL_DESCRIPTION'].value_counts())\n",
        "print(customer['CUSTOMER_SUB_TRADE_CHANNEL_DESCRIPTION'].value_counts())\n",
        "\n"
      ],
      "metadata": {
        "id": "jVhAi7mwU8xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The customer dataset represents 44 unique sales offices, showing extensive geographic coverage across Swire Coca-Cola’s operational regions.\n",
        "- Distribution is dominated by OFS, Sideload, and Rapid Delivery modes, while shipping is highly standardized, with 95% of customers operating on a 48-hour delivery schedule.\n",
        "- The primary cold drink channels are Restaurants and Stores\n",
        "- Further segmentation reveals over 50 customer sub-trade channels, led by Dining, Restaurants, and Local Convenience Stores."
      ],
      "metadata": {
        "id": "O6mgP11oZxto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales.head()"
      ],
      "metadata": {
        "id": "6kG-rTQlqzPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "office_counts = (\n",
        "    customer.groupby(\"SALES_OFFICE_DESCRIPTION\")[\"CUSTOMER_NUMBER\"]\n",
        "    .nunique()\n",
        "    .sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.barplot(x=office_counts.values, y=office_counts.index, palette=\"Reds_r\")\n",
        "plt.title(\"Number of Unique Customers by Sales Office\", fontsize=14)\n",
        "plt.xlabel(\"Customer Count\")\n",
        "plt.ylabel(\"Sales Office\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l06cpU_OdBsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dist_mode_channel = pd.crosstab(\n",
        "    customer['DISTRIBUTION_MODE_DESCRIPTION'],\n",
        "    customer['COLD_DRINK_CHANNEL_DESCRIPTION']\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(dist_mode_channel, annot=True, fmt='d', cmap='YlOrRd')\n",
        "plt.title(\"Distribution Mode by Cold Drink Channel\", fontsize=14)\n",
        "plt.xlabel(\"Cold Drink Channel\")\n",
        "plt.ylabel(\"Distribution Mode\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1nenPfRGamh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- heatmap illustrates that OFSand Sideload are the dominant delivery modes across all cold drink channels, especially for Restaurants and Stores.\n",
        "- Rapid Delivery is mainly used by Restaurants, Hot Beverage, and Attraction customers. This suggests that most customers are served through standard delivery modes, with only a few channels relying on faster or specialized logistics."
      ],
      "metadata": {
        "id": "Eo6DFPa5AqQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(google_analytics.head())\n",
        "\n",
        "print(google_analytics.info())\n",
        "print(google_analytics.shape)\n"
      ],
      "metadata": {
        "id": "tPaitjbVrDbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Convert EVENT_DATE and EVENT_TIMESTAMP to datetime\n",
        "google_analytics['EVENT_DATE'] = pd.to_datetime(google_analytics['EVENT_DATE'])\n",
        "google_analytics['EVENT_TIMESTAMP'] = pd.to_datetime(google_analytics['EVENT_TIMESTAMP'])\n",
        "\n",
        "# Check conversion\n",
        "print(google_analytics[['EVENT_DATE', 'EVENT_TIMESTAMP']].head())\n"
      ],
      "metadata": {
        "id": "H99eeQz_HYX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Replace google_analytics placeholders with NaN\n",
        "google_analytics.replace([\"(not set)\", \"\"], np.nan, inplace=True)\n",
        "\n",
        "# Check missing values per column\n",
        "missing_summary = google_analytics.isnull().sum()\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(missing_summary)\n"
      ],
      "metadata": {
        "id": "dy8QB-EAH0ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count duplicates before removing\n",
        "print(\"Number of duplicate rows:\", google_analytics.duplicated().sum())\n",
        "\n",
        "# Drop duplicates\n",
        "google_analytics = google_analytics.drop_duplicates()\n",
        "\n",
        "# Confirm removal\n",
        "print(\"Rows after removing duplicates:\", len(google_analytics))\n"
      ],
      "metadata": {
        "id": "1ZqwASnDLiG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic info\n",
        "print(\"\\nDataset shape:\", google_analytics.shape)\n",
        "print(\"\\nData types:\")\n",
        "print(google_analytics.dtypes)\n",
        "\n",
        "# Quick glance at the first few rows\n",
        "print(\"\\nHead of dataset:\")\n",
        "print(google_analytics.head())\n"
      ],
      "metadata": {
        "id": "W4KDlgLzLxjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_events = google_analytics['EVENT_NAME'].value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=top_events.values, y=top_events.index, palette=\"viridis\")\n",
        "plt.title(\"Top 10 Google Analytics Events\")\n",
        "plt.xlabel(\"Number of Events\")\n",
        "plt.ylabel(\"Event Name\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HxmSDuopMN4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Most user interactions involve browsing such as page views, item list views, and button clicks, while fewer users progressed to transactional actions like add_to_cart and proceed_to_checkout."
      ],
      "metadata": {
        "id": "WELtIQrrPYMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Device category counts\n",
        "print(\"\\nDevice categories:\")\n",
        "print(google_analytics['DEVICE_CATEGORY'].value_counts())\n",
        "\n",
        "# Top mobile brands\n",
        "print(\"\\nTop 10 mobile brands:\")\n",
        "print(google_analytics['DEVICE_MOBILE_BRAND_NAME'].value_counts().head(10))\n",
        "\n",
        "# Operating systems\n",
        "print(\"\\nOperating systems:\")\n",
        "print(google_analytics['DEVICE_OPERATING_SYSTEM'].value_counts())\n"
      ],
      "metadata": {
        "id": "QYXp_XVjL-vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Most users access the site from desktops, primarily on Windows and Macintosh, while mobile users mainly on Google/Android and Apple/iOS make up a smaller portion."
      ],
      "metadata": {
        "id": "DS6Sq75MNH9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "events_per_day = google_analytics.groupby('EVENT_DATE')['EVENT_NAME'].count()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "events_per_day.plot(kind='line', marker='o')\n",
        "plt.title(\"Daily Events Trend\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Events\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yDKV39m4MeMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The daily events trend shows a steady increase in user activity on the MyCoke360 platform from June 2024 through May 2025\n",
        "- The upward trajectory suggests growing platform adoption and higher engagement"
      ],
      "metadata": {
        "id": "TL4bnkdaPkze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(orders.head())\n",
        "print(orders.info())\n",
        "print(orders.shape)"
      ],
      "metadata": {
        "id": "0pjBA8scrK3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check missing values\n",
        "print(orders.isnull().sum())\n",
        "\n",
        "# Summary of duplicates\n",
        "print(orders.duplicated().sum())\n",
        "\n",
        "# Count distinct customers and products\n",
        "print(orders['CUSTOMER_ID'].nunique(), orders['MATERIAL_ID'].nunique())\n"
      ],
      "metadata": {
        "id": "APgb3t1VRgF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#orders distribution\n",
        "print(orders['ORDER_TYPE'].value_counts(normalize=True) * 100)\n"
      ],
      "metadata": {
        "id": "dzeldx9OSDvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Most orders (61.4%) are placed through Sales Representatives, while MyCoke360 accounts for 27.4%"
      ],
      "metadata": {
        "id": "wZgJW-cHSiex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders['ORDER_QUANTITY'].describe()\n",
        "\n",
        "# Top ordered products\n",
        "orders.groupby('MATERIAL_ID')['ORDER_QUANTITY'].sum().sort_values(ascending=False).head(10)\n"
      ],
      "metadata": {
        "id": "RwkAGr6BScxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert dates\n",
        "orders['CREATED_DATE_EST'] = pd.to_datetime(orders['CREATED_DATE_EST'])\n",
        "\n",
        "# Group by date and order type\n",
        "order_trend = (\n",
        "    orders.groupby(['CREATED_DATE_EST', 'ORDER_TYPE'])\n",
        "    .size()\n",
        "    .reset_index(name='Order_Count')\n",
        ")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.lineplot(\n",
        "    data=order_trend,\n",
        "    x='CREATED_DATE_EST',\n",
        "    y='Order_Count',\n",
        "    hue='ORDER_TYPE',\n",
        "    marker='o'\n",
        ")\n",
        "plt.title('Daily Order Volume by Channel (Sales Rep vs Call Center)', fontsize=14)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Orders')\n",
        "plt.legend(title='Order Type')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tZI1F0EZS4Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sales Representatives drive the majority of daily orders, showing strong and consistent weekly peaks.\n",
        "\n",
        "- MyCoke360 orders are steadily increasing."
      ],
      "metadata": {
        "id": "1GIgMfS9Tnje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(operating_hours.head())"
      ],
      "metadata": {
        "id": "LWj-Hc5YrWsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert dates\n",
        "operating_hours['CALLING_ANCHOR_DATE'] = pd.to_datetime(operating_hours['CALLING_ANCHOR_DATE'], errors='coerce')\n",
        "\n",
        "# Missing values\n",
        "print(\"Missing values per column:\")\n",
        "print(operating_hours.isnull().sum())\n",
        "\n",
        "# Duplicates\n",
        "print(\"\\nNumber of duplicate rows:\")\n",
        "print(operating_hours.duplicated().sum())\n",
        "#frequency\n",
        "print(operating_hours['FREQUENCY'].value_counts())"
      ],
      "metadata": {
        "id": "8DvrEoi539HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "operating_hours['FREQUENCY'].value_counts().plot(kind='bar', figsize=(8,4), title='Operating hours Distribution ')\n"
      ],
      "metadata": {
        "id": "iqxVrO2f-QCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Most operations occur on a regular schedule, with the majority happening every 4 weeks (2,562 records), followed by weekly operations (2,140 records), and every 2 weeks (1,489 records). Only a small portion, 11 records, follow an every 3 weeks pattern."
      ],
      "metadata": {
        "id": "LsdF6tWz-u4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(operating_hours['DELIVERY_ANCHOR_DAY'].value_counts())\n",
        "\n",
        "# Crosstab: Frequency vs Day\n",
        "print(pd.crosstab(operating_hours['DELIVERY_ANCHOR_DAY'], operating_hours['FREQUENCY']))\n"
      ],
      "metadata": {
        "id": "AosO0TQj-8AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Most deliveries occur on weekdays, particularly between Tuesday and Friday, with very few scheduled for weekends"
      ],
      "metadata": {
        "id": "W3zqZyQNAFvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Customers scheduled for each date\n",
        "print(operating_hours.groupby(\"CALLING_ANCHOR_DATE\")['CUSTOMER_NUMBER'].count().plot(figsize=(12,6), title=\"Customers Scheduled per Date\"))\n",
        "\n",
        "# Weekly distribution\n",
        "print(operating_hours['CALLING_ANCHOR_DATE'].dt.day_name().value_counts())\n"
      ],
      "metadata": {
        "id": "jy9xc5yXAmjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The dataset is heavily concentrated around Monday through Wednesday and Sunday, with minimal or almost no calls on Thursday and Friday."
      ],
      "metadata": {
        "id": "MBkVBRsaChoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Customers with multiple delivery records\n",
        "print(operating_hours.groupby(\"CUSTOMER_NUMBER\")['CALLING_ANCHOR_DATE'].count().sort_values(ascending=False).head(10))\n"
      ],
      "metadata": {
        "id": "Fk3tAYrHCmeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cutoff_times.head())\n",
        "print(cutoff_times.info())\n",
        "print(cutoff_times.shape)"
      ],
      "metadata": {
        "id": "yH97S2d0rtS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Optional: make plots look nicer\n",
        "sns.set(style=\"whitegrid\")\n"
      ],
      "metadata": {
        "id": "ITVqalsvapyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show raw values exactly as stored\n",
        "for val in cutoff_times['CUTOFFTIME__C'].head(20):\n",
        "    print(repr(val))\n"
      ],
      "metadata": {
        "id": "i8YZh77tby3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cutoff_times['CUTOFFTIME_CLEAN'] = cutoff_times['CUTOFFTIME__C'].astype(str).str.strip()\n",
        "\n",
        "# Convert to datetime\n",
        "cutoff_times['CUTOFFTIME_DT'] = pd.to_datetime(cutoff_times['CUTOFFTIME_CLEAN'], errors='coerce')\n",
        "\n",
        "# Extract only the time\n",
        "cutoff_times['CUTOFFTIME_ONLY'] = cutoff_times['CUTOFFTIME_DT'].dt.time\n",
        "\n",
        "# Check results\n",
        "print(\"Successfully parsed times:\", cutoff_times['CUTOFFTIME_ONLY'].notnull().sum())\n",
        "print(\"Failed parsing:\", cutoff_times['CUTOFFTIME_ONLY'].isnull().sum())\n",
        "\n",
        "# Inspect first few rows\n",
        "print(cutoff_times[['CUTOFFTIME__C', 'CUTOFFTIME_ONLY']].head(10))\n"
      ],
      "metadata": {
        "id": "XAuTTXFFcqc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cutoff_times['CUTOFFTIME_ONLY'].value_counts().head(10))\n"
      ],
      "metadata": {
        "id": "RibLPwozdEif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode_per_plant = cutoff_times.groupby('PLANT_ID')['CUTOFFTIME_ONLY'].agg(lambda x: x.mode()[0])\n",
        "print(mode_per_plant)\n"
      ],
      "metadata": {
        "id": "cL6dijK2dU6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'SHIPPING_CONDITION_TIME' to numeric hours\n",
        "cutoff_times['SHIPPING_HOURS'] = cutoff_times['SHIPPING_CONDITION_TIME'].str.replace(\"hrs\",\"\").astype(int)\n",
        "\n",
        "# Quick check\n",
        "print(cutoff_times[['SHIPPING_CONDITION_TIME', 'SHIPPING_HOURS']].head())\n"
      ],
      "metadata": {
        "id": "vpySNdJ4dxtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cutoff_times['SHIPPING_HOURS'].describe()\n",
        "cutoff_times['SHIPPING_HOURS'].hist(figsize=(8,5))"
      ],
      "metadata": {
        "id": "WKeqaQOueR5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.boxplot(x='CUTOFFTIME_ONLY', y='SHIPPING_HOURS', data=cutoff_times)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Shipping Hours vs Cutoff Times\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0awTcPY1etcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KsJxZFdNcY_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The submission deadlines vary between 10:00 AM and 4:00 PM, with the most frequent cutoff windows occurring around 3:00 PM (36), 3:30 PM (27), and 4:00 PM (26).\n",
        "- These times suggest a strong operational clustering in the mid-afternoon period.\n",
        "- The earlier cutoff times are associated with shorter shipping durations (24–30 hours), whereas later cutoff times correspond to longer shipping windows (48–72 hours).\n",
        "- This pattern suggests that regional variations in cutoff scheduling directly influence fulfillment efficiency, where later submission allowances tend to delay shipment cycles."
      ],
      "metadata": {
        "id": "cssn3BMVgGSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(materials.head())\n",
        "print(materials.info())\n",
        "print(materials.shape)"
      ],
      "metadata": {
        "id": "5G2AxF39r07r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(materials.isnull().sum())\n",
        "print(materials.duplicated().sum())\n",
        "print(materials.nunique())"
      ],
      "metadata": {
        "id": "AX75BC4-VR4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fill missing BEV_CAT_DESC values\n",
        "materials['BEV_CAT_DESC'] = materials['BEV_CAT_DESC'].fillna('Unknown')\n"
      ],
      "metadata": {
        "id": "27OFPXC9Vh0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "materials['BEV_CAT_DESC'].value_counts(normalize=True)*100\n"
      ],
      "metadata": {
        "id": "ZnZTUZDkVtH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The beverage portfolio is dominated by Core Sparkling products (36.2%), followed by Energy Drinks (22.5%) and Sports Drinks (11.1%), together accounting for nearly 70% of all SKUs."
      ],
      "metadata": {
        "id": "gA4_iq5AWMZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(y='TRADE_MARK_DESC', data=materials, order=materials['TRADE_MARK_DESC'].value_counts().head(10).index)\n",
        "plt.title('Top 10 Brands by Product Count')\n"
      ],
      "metadata": {
        "id": "Qh9UhlyZWjmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(materials['BEV_CAT_DESC'], materials['PACK_TYPE_DESC'])\n"
      ],
      "metadata": {
        "id": "uCtzW_gOWwZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(visit_plan.head())\n",
        "print(visit_plan.info())\n",
        "print(visit_plan.shape)"
      ],
      "metadata": {
        "id": "KVwxSQyzsAQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(visit_plan.isnull().sum())\n",
        "print(visit_plan.duplicated().sum())\n",
        "print(visit_plan.nunique())\n"
      ],
      "metadata": {
        "id": "eECM84wtXgVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visit_plan['FREQUENCY'].value_counts().plot(kind='bar', figsize=(8,4), title='Distribution of Visit Frequencies')\n"
      ],
      "metadata": {
        "id": "NVQ5G-yVYfYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The majority of customers are visited weekly\n",
        "\n",
        "- Less frequent visits (bi-weekly, monthly, or quarterly) occur for smaller or lower-volume customers"
      ],
      "metadata": {
        "id": "cMGSz1eBZ14G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(y='SALES_OFFICE_DESC', data=visit_plan,\n",
        "              order=visit_plan['SALES_OFFICE_DESC'].value_counts().head(10).index)\n",
        "plt.title('Top 10 Sales Offices by Planned Visits')\n"
      ],
      "metadata": {
        "id": "MZ5vBg5UYz4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Draper, UT handles the highest number of planned visits,over 2.5 million, followed by Tempe, AZ and Denver, showing these are major operational hubs.\n",
        "- Top offices likely serve the largest customer bases or most active routes,"
      ],
      "metadata": {
        "id": "lr8NTwhMZjhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(visit_plan['DISTRIBUTION_MODE'], visit_plan['SHIPPING_CONDITIONS_DESC'])\n"
      ],
      "metadata": {
        "id": "wc4LbSZoY_4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Most deliveries use 48-hour shipping, dominated by Sideload (SL), Off-Floor (OF), and Route Delivery (RD) modes, showing these as the main distribution channels.\n",
        "\n",
        "- 24-hour shipments occur less frequently and are concentrated in Route Delivery and Sideload, indicating faster service for select regions or customers."
      ],
      "metadata": {
        "id": "HVmue8r-aUh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Key EDA Insights\n",
        "\n",
        "- The EDA revealed strong operational consistency across Swire Coca-Cola’s network, with most customers served weekly through 48-hour shipping windows dominated by Sideload and Off-Floor modes.\n",
        "- Desktop usage is dominant; mobile activity is less frequent.  \n",
        "- Distribution modes (OFS, Sideload) strongly influence ordering patterns.  \n",
        "- Cutoff times cluster around mid-afternoon, creating pressure that may lead to incomplete orders.  \n",
        "- Sales Representatives handled the majority of orders, while MyCoke360 showed growing digital adoption.\n",
        "- Google Analytics indicated high browsing engagement but lower checkout conversion, supporting the cart abandonment concern.\n",
        "\n",
        " Overall, there is clear behavioral and logistical patterns,providing foundation for predicting at-risk customers and improving order completion on the MyCoke360 platform."
      ],
      "metadata": {
        "id": "h9selpYPaiXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Modeling\n",
        "\n",
        "## Target Definition  \n",
        "The target variable made_a_purchase equals 1 if an order was completed in the defined window.\n",
        "\n",
        "## Leakage Handling  \n",
        "To ensure valid predictions, I removed all features that may contain leakage, including:  \n",
        "- Purchase success indicators  \n",
        "- Completion timestamps  \n",
        "- Post-checkout event pages  \n",
        "- Flags explicitly encoding purchase outcome  \n",
        "\n",
        "## Feature Engineering  \n",
        "Key features engineered include:\n",
        "- Time features (hour, day-of-week, month)  \n",
        "- Device category indicators  \n",
        "- Event-level behavioral signals (add_to_cart, remove_from_cart, etc.)  \n",
        "- Item count extracted from JSON lists  \n",
        "\n",
        "##  Train/Test Split  \n",
        "To simulate real deployment, I used a **time-based split**:\n",
        "- First 75% of events → training  \n",
        "- Last 25% → holdout testing  \n"
      ],
      "metadata": {
        "id": "06ynY-N1iwkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Loading modelling dataset\n",
        "file_path = \"/content/drive/MyDrive/Final Output for Modeling .csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(\"Raw shape:\", df.shape)\n",
        "display(df.head())\n",
        "\n",
        "df.info()\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isna().sum())\n",
        "\n",
        "display(df.describe(include='all').transpose())\n"
      ],
      "metadata": {
        "id": "nlGFvTtsDk_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import ast\n",
        "\n",
        "#Normalize placeholder strings to NaN\n",
        "PLACEHOLDERS = {\n",
        "    \"not available given order type\",\n",
        "    \"not available\",\n",
        "    \"NA\", \"N/A\", \"None\", \"\", \"null\", \"NULL\"\n",
        "}\n",
        "\n",
        "df_clean = df.copy()\n",
        "\n",
        "for col in df_clean.columns:\n",
        "    if df_clean[col].dtype == \"object\":\n",
        "        df_clean[col] = df_clean[col].replace(list(PLACEHOLDERS), np.nan)\n",
        "\n",
        "# Parse datetime columns if present\n",
        "def to_dt(x):\n",
        "    return pd.to_datetime(x, errors=\"coerce\")\n",
        "\n",
        "dt_cols = [\"transaction_date_df2\", \"event_date_df1\", \"EVENT_TIMESTAMP\"]\n",
        "for c in dt_cols:\n",
        "    if c in df_clean.columns:\n",
        "        df_clean[c] = to_dt(df_clean[c])\n",
        "\n",
        "# Coerce numeric candidates\n",
        "def to_num(s):\n",
        "    return pd.to_numeric(s, errors=\"coerce\")\n",
        "\n",
        "num_candidates = [\"ORDER_QUANTITY\", \"MATERIAL_ID\"]\n",
        "for c in num_candidates:\n",
        "    if c in df_clean.columns and df_clean[c].dtype == \"object\":\n",
        "        df_clean[c] = to_num(df_clean[c])\n",
        "\n",
        "# define target\n",
        "if \"made_a_purchase\" not in df_clean.columns:\n",
        "    raise ValueError(\"Expected 'made_a_purchase' column as target.\")\n",
        "df_clean[\"made_a_purchase\"] = df_clean[\"made_a_purchase\"].fillna(0).astype(int)\n",
        "\n",
        "print(\"Cleaned frame shape:\", df_clean.shape)\n",
        "df_clean[[\"made_a_purchase\"]].value_counts(normalize=True).rename(\"proportion\")\n"
      ],
      "metadata": {
        "id": "CNIBLkM4DqvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Engineering\n",
        "\n",
        "#Transaction-based time features\n",
        "if \"transaction_date_df2\" in df_clean.columns:\n",
        "    df_clean[\"trans_dow\"] = df_clean[\"transaction_date_df2\"].dt.dayofweek\n",
        "    df_clean[\"trans_hour\"] = df_clean[\"transaction_date_df2\"].dt.hour\n",
        "    df_clean[\"trans_month\"] = df_clean[\"transaction_date_df2\"].dt.month\n",
        "    df_clean[\"trans_is_weekend\"] = df_clean[\"trans_dow\"].isin([5, 6]).astype(int)\n",
        "\n",
        "# Event-based date/time features\n",
        "if \"event_date_df1\" in df_clean.columns:\n",
        "    df_clean[\"event_dow\"] = df_clean[\"event_date_df1\"].dt.dayofweek\n",
        "    df_clean[\"event_month\"] = df_clean[\"event_date_df1\"].dt.month\n",
        "if \"EVENT_TIMESTAMP\" in df_clean.columns:\n",
        "    df_clean[\"event_hour\"] = df_clean[\"EVENT_TIMESTAMP\"].dt.hour\n",
        "\n",
        "# ITEMS feature\n",
        "def items_count(val):\n",
        "    if pd.isna(val):\n",
        "        return np.nan\n",
        "    try:\n",
        "        parsed = ast.literal_eval(val)\n",
        "        if isinstance(parsed, list):\n",
        "            return len(parsed)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return np.nan\n",
        "\n",
        "if \"ITEMS\" in df_clean.columns and df_clean[\"ITEMS\"].dtype == \"object\":\n",
        "    df_clean[\"items_count\"] = df_clean[\"ITEMS\"].apply(items_count)\n",
        "\n",
        "# Device/channel features\n",
        "if \"DEVICE_CATEGORY\" in df_clean.columns:\n",
        "    df_clean[\"is_mobile\"] = df_clean[\"DEVICE_CATEGORY\"].isin([\"mobile\", \"tablet\"]).astype(int)\n",
        "    df_clean[\"is_desktop\"] = (df_clean[\"DEVICE_CATEGORY\"] == \"desktop\").astype(int)\n",
        "\n",
        "# Event action flags\n",
        "if \"EVENT_NAME\" in df_clean.columns:\n",
        "    df_clean[\"evt_add_to_cart\"] = (df_clean[\"EVENT_NAME\"] == \"add_to_cart\").astype(int)\n",
        "    df_clean[\"evt_begin_checkout\"] = df_clean[\"EVENT_NAME\"].isin(\n",
        "        [\"begin_checkout\", \"proceed_to_checkout\"]\n",
        "    ).astype(int)\n",
        "    df_clean[\"evt_purchase\"] = (df_clean[\"EVENT_NAME\"] == \"purchase\").astype(int)\n",
        "    df_clean[\"evt_remove_from_cart\"] = (df_clean[\"EVENT_NAME\"] == \"remove_from_cart\").astype(int)\n",
        "\n",
        "print(\"After feature engineering:\", df_clean.shape)\n",
        "display(df_clean.head())\n"
      ],
      "metadata": {
        "id": "J476acmiDvvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Addressing Obvious Leakage\n",
        "\n",
        "# Dropping columns that directly encode success / outcome / post-purchase info\n",
        "possible_leakage_cols = [\n",
        "    'purchase_successful',\n",
        "    'EVENT_PAGE_NAME',\n",
        "    'checkout_result',\n",
        "    'order_status',\n",
        "    'completed_flag',\n",
        "    'transaction_id',\n",
        "    'final_order_value',\n",
        "    'payment_success',\n",
        "    'is_converted',\n",
        "    'order_complete_flag',\n",
        "    'converted',\n",
        "    'purchase_timestamp',\n",
        "    'abandoned_in_window',\n",
        "    'events_full_window',\n",
        "    'actions_after_deadline',\n",
        "    'evt_purchase',\n",
        "]\n",
        "\n",
        "df_clean = df_clean.drop(\n",
        "    columns=[c for c in possible_leakage_cols if c in df_clean.columns],\n",
        "    errors=\"ignore\"\n",
        ")\n",
        "\n",
        "print(\"Columns after dropping obvious leakage:\")\n",
        "print(sorted(df_clean.columns))\n",
        "\n",
        "# Removing explicit purchase rows from event-level data\n",
        "work = df_clean.copy()\n",
        "\n",
        "if \"EVENT_NAME\" in work.columns:\n",
        "    work = work[work[\"EVENT_NAME\"].ne(\"purchase\")]\n",
        "\n",
        "# Also rows whose event page looks like a success page\n",
        "if \"EVENT_PAGE_NAME\" in work.columns:\n",
        "    work = work[~work[\"EVENT_PAGE_NAME\"].fillna(\"\").str.contains(\"purchase success\", case=False)]\n",
        "\n",
        "# Making sure target stays intact\n",
        "work[\"made_a_purchase\"] = work[\"made_a_purchase\"].fillna(0).astype(int)\n",
        "\n",
        "print(\"Leak-filtered working frame shape:\", work.shape)\n",
        "display(work.head())\n"
      ],
      "metadata": {
        "id": "HFtSxDBnD2cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "txgI8FEAlGlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modeling Frame\n",
        "# categorical features\n",
        "safe_categorical = [c for c in [\n",
        "    \"EVENT_NAME\",\n",
        "    \"DEVICE_CATEGORY\",\n",
        "    \"DEVICE_MOBILE_BRAND_NAME\",\n",
        "    \"DEVICE_OPERATING_SYSTEM\",\n",
        "    \"ORDER_TYPE\",\n",
        "] if c in work.columns]\n",
        "\n",
        "# engineered numeric features\n",
        "safe_numeric = [c for c in [\n",
        "    \"items_count\",\n",
        "    \"trans_dow\", \"trans_hour\", \"trans_month\", \"trans_is_weekend\",\n",
        "    \"event_dow\", \"event_hour\", \"event_month\",\n",
        "    \"is_mobile\", \"is_desktop\",\n",
        "    \"evt_add_to_cart\", \"evt_begin_checkout\", \"evt_remove_from_cart\"\n",
        "] if c in work.columns]\n",
        "\n",
        "id_cols = [c for c in [\"CUSTOMER_ID\"] if c in work.columns]\n",
        "\n",
        "cols_needed = (\n",
        "    id_cols +\n",
        "    ([\"event_date_df1\"] if \"event_date_df1\" in work.columns else []) +\n",
        "    safe_categorical + safe_numeric +\n",
        "    [\"made_a_purchase\"]\n",
        ")\n",
        "\n",
        "df_leakfree = work[cols_needed].copy()\n",
        "\n",
        "print(\"Leak-safe modeling frame shape:\", df_leakfree.shape)\n",
        "print(\"\\nTarget distribution (made_a_purchase):\")\n",
        "print(df_leakfree[\"made_a_purchase\"].value_counts(normalize=True).rename(\"proportion\"))\n",
        "\n",
        "display(df_leakfree.head())\n"
      ],
      "metadata": {
        "id": "vGs0dwWwD-sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train/Test Split (Time-Based Holdout)\n",
        "\n",
        "from sklearn.model_selection import GroupKFold, KFold\n",
        "import pandas.api.types as ptypes\n",
        "\n",
        "# Separate X and y\n",
        "X = df_leakfree.drop(\n",
        "    columns=[\"made_a_purchase\", \"event_date_df1\"] if \"event_date_df1\" in df_leakfree.columns else [\"made_a_purchase\"]\n",
        ")\n",
        "y = df_leakfree[\"made_a_purchase\"].copy()\n",
        "\n",
        "# Group labels\n",
        "groups = df_leakfree[\"CUSTOMER_ID\"] if \"CUSTOMER_ID\" in df_leakfree.columns else None\n",
        "\n",
        "# Time-based holdout: last 25% by event_date_df1; fallback to index order\n",
        "if \"event_date_df1\" in df_leakfree.columns and ptypes.is_datetime64_any_dtype(df_leakfree[\"event_date_df1\"]):\n",
        "    order_idx = np.argsort(df_leakfree[\"event_date_df1\"].values)\n",
        "    cutoff = int(len(order_idx) * 0.75)\n",
        "    train_idx, test_idx = order_idx[:cutoff], order_idx[cutoff:]\n",
        "else:\n",
        "    cutoff = int(len(X) * 0.75)\n",
        "    train_idx, test_idx = np.arange(cutoff), np.arange(cutoff, len(X))\n",
        "\n",
        "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "groups_train = groups.iloc[train_idx] if groups is not None else None\n",
        "\n",
        "print(\"Train/Test shapes:\", X_train.shape, X_test.shape)\n",
        "print(\"\\nTarget distribution (train):\")\n",
        "print(y_train.value_counts(normalize=True).rename(\"proportion\"))\n"
      ],
      "metadata": {
        "id": "sEoPt3ZREFob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bKksUee5lyW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Model Training & Evaluation\n",
        "\n",
        " Logistic regression model was trained using a preprocessed pipeline that handles:\n",
        "- Imputation  \n",
        "- One-hot encoding of categorical variables  \n",
        "- Scaling numeric features  \n",
        "\n",
        "The model was evaluated using:\n",
        "- Cross-validated ROC-AUC  \n",
        "- Time-based holdout ROC-AUC  \n",
        "- Confusion matrix  \n",
        "- Classification report  \n"
      ],
      "metadata": {
        "id": "t5rx2_KLHxAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic Regression Pipeline + CV\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
        "\n",
        "# categorical vs numeric columns\n",
        "cat_cols = [c for c in X_train.columns if X_train[c].dtype == \"object\"]\n",
        "num_cols = [c for c in X_train.columns if X_train[c].dtype != \"object\"]\n",
        "\n",
        "categorical_preprocess = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "numeric_preprocess = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"sc\", StandardScaler())\n",
        "])\n",
        "\n",
        "preproc = ColumnTransformer([\n",
        "    (\"cat\", categorical_preprocess, cat_cols),\n",
        "    (\"num\", numeric_preprocess, num_cols)\n",
        "], remainder=\"drop\")\n",
        "\n",
        "logit = Pipeline([\n",
        "    (\"prep\", preproc),\n",
        "    (\"model\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42))\n",
        "])\n",
        "\n",
        "# Cross-validation on TRAIN\n",
        "if groups_train is not None:\n",
        "    gkf = GroupKFold(n_splits=5)\n",
        "    cv_scores = cross_val_score(\n",
        "        logit, X_train, y_train,\n",
        "        cv=gkf.split(X_train, y_train, groups_train),\n",
        "        scoring=\"roc_auc\"\n",
        "    )\n",
        "else:\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    cv_scores = cross_val_score(logit, X_train, y_train, cv=kf, scoring=\"roc_auc\")\n",
        "\n",
        "print(\"Logistic CV ROC-AUC:\", np.round(cv_scores, 3), \"Mean:\", round(cv_scores.mean(), 3))\n",
        "\n",
        "# Fit on TRAIN and evaluate on time-based TEST\n",
        "logit.fit(X_train, y_train)\n",
        "y_prob = logit.predict_proba(X_test)[:, 1]\n",
        "y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\nTime-based Test ROC-AUC:\", round(roc_auc_score(y_test, y_prob), 3))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, digits=3))\n"
      ],
      "metadata": {
        "id": "zrxMGEPDEPT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dHDeMN4rmB7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  ROC Curve, Brier Score & Calibration\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.metrics import brier_score_loss\n",
        "\n",
        "# ROC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(\"Time-based Test ROC-AUC:\", round(roc_auc, 3))\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.plot(fpr, tpr, label=f\"Logistic (AUC={roc_auc:.3f})\")\n",
        "plt.plot([0,1], [0,1], \"--\", color=\"gray\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve – Purchase Prediction (Leak-safe)\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Brier score\n",
        "brier_model = brier_score_loss(y_test, y_prob)\n",
        "base_prob = y_train.mean()\n",
        "brier_baseline = brier_score_loss(y_test, np.full_like(y_test, fill_value=base_prob, dtype=float))\n",
        "\n",
        "print(\"Brier score (model):\", round(brier_model, 4))\n",
        "print(\"Brier score (baseline, always = avg rate {:.3f}): {:.4f}\".format(base_prob, brier_baseline))\n"
      ],
      "metadata": {
        "id": "OIrI5RU_EXJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sNrW4M4tmbJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Profit-Based Threshold Optimization\n",
        "\n",
        "While classification metrics are useful, Our real objective is **recovering revenue **.  \n",
        "Using assumptions for:\n",
        "- Cost per contact  \n",
        "- Expected profit per recovered purchase  \n",
        "\n",
        "I computed net profit across thresholds from 0.01–0.99.  \n",
        "The optimal threshold maximizes expected profit and provides a deployable decision rule.\n"
      ],
      "metadata": {
        "id": "-lkRy2ICIH2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Profit-Based Threshold Tuning\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assumptions\n",
        "cost_per_contact = 1.0\n",
        "profit_per_purchase = 50.0\n",
        "def profit_curve(y_true, y_prob, cost_per_contact, profit_per_purchase, n_thresholds=50):\n",
        "    thresholds = np.linspace(0.01, 0.99, n_thresholds)\n",
        "    rows = []\n",
        "    n = len(y_true)\n",
        "    for thr in thresholds:\n",
        "        y_pred = (y_prob >= thr).astype(int)\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "        n_targeted = tp + fp\n",
        "\n",
        "        revenue = tp * profit_per_purchase\n",
        "        cost = n_targeted * cost_per_contact\n",
        "        net_profit = revenue - cost\n",
        "        profit_per_1000 = (net_profit / n) * 1000\n",
        "\n",
        "        rows.append({\n",
        "            \"threshold\": thr,\n",
        "            \"tp\": tp,\n",
        "            \"fp\": fp,\n",
        "            \"fn\": fn,\n",
        "            \"tn\": tn,\n",
        "            \"n_targeted\": n_targeted,\n",
        "            \"revenue\": revenue,\n",
        "            \"cost\": cost,\n",
        "            \"net_profit\": net_profit,\n",
        "            \"net_profit_per_1000\": profit_per_1000\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "profit_df = profit_curve(y_test.values, y_prob, cost_per_contact, profit_per_purchase)\n",
        "best_row = profit_df.loc[profit_df[\"net_profit\"].idxmax()]\n",
        "\n",
        "print(\"Best threshold by net profit:\")\n",
        "display(best_row)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(profit_df[\"threshold\"], profit_df[\"net_profit_per_1000\"])\n",
        "plt.axvline(best_row[\"threshold\"], linestyle=\"--\", label=f\"Best thr={best_row['threshold']:.2f}\")\n",
        "plt.title(\"Net Profit per 1,000 Customers vs Threshold\")\n",
        "plt.xlabel(\"Decision Threshold (p̂ ≥ threshold → contact)\")\n",
        "plt.ylabel(\"Net Profit per 1,000 Customers\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z8-BnpqaEbH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Final Recommendations & Next Steps\n",
        "\n",
        "- Target customers above the optimal probability threshold for outreach, maximizing revenue recovery.  \n",
        "- Focus user-experience improvements on the most common drop-off points identified in GA events.    \n",
        "- Consider A/B testing different outreach strategies for customers at varying risk levels.  \n"
      ],
      "metadata": {
        "id": "pUwwBmPpIlm9"
      }
    }
  ]
}